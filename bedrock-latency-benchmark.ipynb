{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6c18653-9acf-4aa5-9800-76dadb55a338",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark latency for Amazon Bedrock models\n",
    "Use this notebook to measure model latency in Amazon Bedrock.  \n",
    "Start by:\n",
    "1. Specifying scenarios to test (`scenarios`)\n",
    "2. Execute benchmark\n",
    "3. Reporting - Observe results statistics as data and graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0233b6f7-1996-4fa5-8378-6e88bd64cd73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specifying scenarios to test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f10c830-71d3-4253-8b7d-74e372f3f394",
   "metadata": {},
   "source": [
    "Here are various possible scenarios to benchmark.\n",
    "Each scenario is a dictionary with latency relevant keys:\n",
    "- `model_id` - the model to test, smaller models are likely slower. Currently only Anthropic models are supported.\n",
    "- `in_tokens` - the number of tokens to feed to the model. aka: input context length. Range: 40 - 100K.\n",
    "- `out_tokens` - the number of tokens for the model to generate. Range: 1 - 8191.\n",
    "- `region` - The AWS region to invoke Bedrock in. This can affect network latency depending on client location.\n",
    "- `stream` - True|False - A streaming response starts returning tokens to the client as they are generated, instead of waiting before returning the complete resopnses. This should be True for interactive use cases.\n",
    "- `name` - a human readable name for the scenario (will appear in reports and graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69b1a3-c8a0-47df-9dca-2e8dda9164c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cases_scenarios = [\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-v2',\n",
    "                'in_tokens'  : 1000,\n",
    "                'out_tokens' : 200,\n",
    "                'region'     : 'us-east-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'Summarization. in=1000, out=200',\n",
    "            },\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-v2',\n",
    "                'in_tokens' : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'us-east-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'Classification. in=200, out=50',\n",
    "            },\n",
    "]\n",
    "\n",
    "region_compare_scenarios = [\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-instant-v1',\n",
    "                'in_tokens'  : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'us-east-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'us-east-1',\n",
    "            },\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-instant-v1',\n",
    "                'in_tokens' : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'us-west-2',\n",
    "                'stream' : True,\n",
    "                'name' : f'us-west-2',\n",
    "            },\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-instant-v1',\n",
    "                'in_tokens' : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'eu-central-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'eu-central-1',\n",
    "            },\n",
    "]\n",
    "\n",
    "model_compare_scenarios = [\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-v2',\n",
    "                'in_tokens'  : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'us-east-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'claude-v2. in=200, out=50',\n",
    "            },\n",
    "            {\n",
    "                'model_id'    : 'anthropic.claude-instant-v1',\n",
    "                'in_tokens' : 200,\n",
    "                'out_tokens' : 50,\n",
    "                'region'     : 'us-east-1',\n",
    "                'stream' : True,\n",
    "                'name' : f'claude-instant-v1. in=200, out=50',\n",
    "            },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905385c4",
   "metadata": {},
   "source": [
    "Uncomment your desired scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the scenario you want:\n",
    "#scenarios = use_cases_scenarios.copy()\n",
    "scenarios = region_compare_scenarios.copy()\n",
    "#scenarios = model_compare_scenarios.copy()\n",
    "\n",
    "# The number of times to benchmark each scenario. \n",
    "# This is important in measuring variance and average response time across a long duration.\n",
    "invocations_per_scenario = 2 \n",
    "\n",
    "# Seconds to sleep between each invocation. (0 is no sleep). Sleeping between invocation can help you measure across longer periods of time, and/or avoid throttling.\n",
    "sleep_between_invocations = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "201e7b19-7c12-4d8a-a8b3-0fc792cec9d5",
   "metadata": {},
   "source": [
    "# Install needed dependencies\n",
    "This notebook requires a Python 3 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0547b7e-8934-431c-8527-7c0175766bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet --upgrade boto3 awscli matplotlib numpy pandas anthropic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45b8b11d-f238-48cd-b5d3-04800c0f53af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0334e0-8f20-4566-84e3-8a4e857a2e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, botocore, os\n",
    "import random \n",
    "import anthropic\n",
    "\n",
    "anthropic_client = anthropic.Anthropic() # used to count tokens only\n",
    "\n",
    "# This internal method will include arbitrary long input that is designed to generate an extremely long model output\n",
    "def _get_prompt_template(num_input_tokens):\n",
    "    tokens = 'Human:'\n",
    "    tokens += 'Ignore X' + '<X>'\n",
    "    for i in range(num_input_tokens-1):\n",
    "        tokens += random.choice(['hello', 'world', 'foo', 'bar']) + ' '\n",
    "    tokens += '</X>'\n",
    "    tokens += \"print numbers 1 to 9999 as words. don't omit for brevity\"\n",
    "    tokens += '\\n\\nAssistant:one two'  # model will continue with \" three four five...\"\n",
    "    return tokens\n",
    "\n",
    "''' \n",
    "This method creates a prompt of input length `expected_num_tokens` which instructs the LLM to generate extremely long model resopnse\n",
    "'''\n",
    "def create_prompt(expected_num_tokens):\n",
    "    num_tokens_in_prompt_template = anthropic_client.count_tokens(_get_prompt_template(0))\n",
    "    additional_tokens_needed = max(expected_num_tokens - num_tokens_in_prompt_template,0)\n",
    "    \n",
    "    prompt_template = _get_prompt_template(additional_tokens_needed)\n",
    "    \n",
    "    actual_num_tokens = anthropic_client.count_tokens(prompt_template)\n",
    "    #print(f'expected_num_tokens={expected_num_tokens}, actual_tokens={actual_num_tokens}')\n",
    "    assert expected_num_tokens==actual_num_tokens, f'Failed to generate prompt at required length: expected_num_tokens{expected_num_tokens} != actual_num_tokens={actual_num_tokens}'\n",
    "    \n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ffb1d-3f57-4c88-9f14-38be37dc8f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, json\n",
    "from botocore.exceptions import ClientError\n",
    "sleep_on_throttling_sec = 5\n",
    "\n",
    "def benchmark(bedrock, prompt, max_tokens_to_sample, stream=True, temprature=0):\n",
    "    modelId = 'anthropic.claude-v2'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    \n",
    "    body = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens_to_sample\": max_tokens_to_sample,\n",
    "    \"temperature\": 0,\n",
    "})\n",
    "    while True:\n",
    "        try:\n",
    "            start = time.time()\n",
    "\n",
    "            if stream:\n",
    "                response = bedrock.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "            else:\n",
    "                response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "            #print(response)\n",
    "            \n",
    "            first_byte = None\n",
    "            if stream:\n",
    "                event_stream = response.get('body')\n",
    "                for event in event_stream:\n",
    "                    chunk = event.get('chunk')\n",
    "                    if chunk:\n",
    "                        if not first_byte:\n",
    "                            first_byte = time.time() # update the time to first byte\n",
    "                        #print(f'chunk:\\n {json.loads(chunk.get('bytes').decode())}')\n",
    "                # end of stream - check stop_reson in last chunk\n",
    "                stop_reason = json.loads(chunk.get('bytes').decode())['stop_reason']    \n",
    "                last_byte = time.time()\n",
    "            else:\n",
    "                #no streaming flow\n",
    "                first_byte = time.time()\n",
    "                last_byte = first_byte\n",
    "                response_body = json.loads(response.get('body').read())\n",
    "                stop_reason = response_body['stop_reason']\n",
    "\n",
    "            \n",
    "            # verify we got all of the intended output tokens by verifying stop_reason\n",
    "            assert stop_reason == 'max_tokens', f\"stop_reason is {stop_reason} instead of 'max_tokens', this means the model generated less tokens than required.\"\n",
    "\n",
    "            duration_to_first_byte = first_byte - start\n",
    "            duration_to_last_byte = last_byte - start\n",
    "        except ClientError as err:\n",
    "            if 'Thrott' in err.response['Error']['Code']:\n",
    "                print(f'Got ThrottlingException. Sleeping {sleep_on_throttling_sec} sec and retrying.')\n",
    "                time.sleep(sleep_on_throttling_sec)\n",
    "                continue\n",
    "            raise err\n",
    "        break\n",
    "    return duration_to_first_byte, duration_to_last_byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd25501-d6e8-4039-a87c-ab48c5ff0e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "''' \n",
    "Get a boto3 bedrock runtime client for invoking requests\n",
    "region - the AWS region to use\n",
    "Note: Removing auto retries to ensure we're measuring a single transcation (e.g., in case of throttling).\n",
    "'''\n",
    "def _get_bedrock_client(region, warmup=True):\n",
    "    client = boto3.client( service_name='bedrock-runtime',\n",
    "                          region_name=region,\n",
    "                          config=botocore.config.Config(retries=dict(max_attempts=0))) \n",
    "    if warmup:\n",
    "        benchmark(client, create_prompt(50), 1)\n",
    "    return client\n",
    "\n",
    "'''\n",
    "Get a possible cache client per AWS region \n",
    "'''\n",
    "client_per_region={}\n",
    "def get_cached_client(region):\n",
    "    if client_per_region.get(region) is None:\n",
    "        client_per_region[region] = _get_bedrock_client(region)\n",
    "    return client_per_region[region]\n",
    "\n",
    "\n",
    "def post_iteration(is_last_invocation):\n",
    "    if sleep_between_invocations > 0 and not is_last_invocation:\n",
    "        print(f'Sleeping for {sleep_between_invocations} seconds.')\n",
    "        time.sleep(sleep_between_invocations)\n",
    "        \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deed7243",
   "metadata": {},
   "source": [
    "# Execute the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ec07c-d0d6-423c-8b7f-32b109d02bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "early_break = False # early breaking will break after a single scenario, useful for debugging.\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for i in range(invocations_per_scenario): # increase to sample each use case more than once to discover jitter\n",
    "        try:\n",
    "            prompt = create_prompt(scenario['in_tokens'])\n",
    "            client = get_cached_client(scenario['region'])\n",
    "            time_to_first_token,time_to_last_token = benchmark(client, prompt, scenario['out_tokens'], stream=scenario['stream'])\n",
    "\n",
    "            if 'durations' not in scenario: scenario['durations'] = list()\n",
    "            duration = {\n",
    "                'time-to-first-token':  time_to_first_token,\n",
    "                'time-to-last-token':  time_to_last_token,\n",
    "            }\n",
    "            scenario['durations'].append(duration)\n",
    "\n",
    "            print(f\"Scenario: [{scenario['name']}, \" + \n",
    "                  f'Duration: {pp.pformat((duration))}')\n",
    "            \n",
    "            post_iteration(is_last_invocation = i == invocations_per_scenario - 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Error while processing scenario: {scenario['name']}.\")\n",
    "        if early_break:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5f628d9-3e5d-4501-b0f0-504d636898de",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec03cd0-a5f2-4989-b786-3f0b18e1228a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp.pprint(scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf07c9d-92df-4034-b465-8bbc6423e7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "metric = 'time-to-first-token'\n",
    "#metric = 'time-to-last-token'\n",
    "\n",
    "for scenario in scenarios:\n",
    "  durations = [d[metric] for d in scenario['durations']]\n",
    "  \n",
    "  ax.boxplot(durations, positions=[scenarios.index(scenario)])\n",
    "  \n",
    "  ax.set_xticks(range(len(scenarios)))\n",
    "  ax.set_xticklabels([s['name'] for s in scenarios])\n",
    "  \n",
    "  ax.set_ylabel(f'{metric} (sec)')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
